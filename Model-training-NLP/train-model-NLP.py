import firebase_admin
from firebase_admin import credentials
from firebase_admin import db
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
#save the model
import time
import matplotlib.pyplot as plt
import re
import string
import nltk
import os
import tensorflow as tf

# all these run in colab by google ,

# this is just the review for the training process

pip install -U tfds-nightly
pip install -q -U tensorflow-text
pip install -q -U tf-models-official


import tensorflow_hub as hub
import tensorflow_datasets as tfds
import tensorflow_text as text  # A dependency of the preprocessing model
import tensorflow_addons as tfa
from official.nlp import optimization
import numpy as np
import re

tf.get_logger().setLevel('ERROR')
comment = pd.read_csv('/{your data set}', error_bad_lines=False, encoding="latin-1")

#clean the data
comment_dataset = comment.rename(columns={'ï»¿comment':'comment'})

ps = nltk.PorterStemmer()

#remove stop word
# need to exclude not in the stopword
myStopwords = ['virginamerica',
'x',
'y',
'.',
'.....',
'.',
'+',
'-',
'/',
':',
'"',
'\'',
'Amarillo',
'your',
'yours',
'yourself',
'yourselves',
'you',
'yond',
'yonder',
'yon',
'ye',
'z',
'zillion',
'j',
'u',
'umpteen',
'usually'
'us',
'username',
'uponed',
'upons',
'uponing',
'upon',
'ups',
'upping',
'upped',
'up',
'unto',
'until',
'unliker',
'unlikest',
'under'
'underneath'
'use'
'used'
'usedest'
'r',
'rath',
'rather',
'rathest',
'rathe',
're',
'relate',
'related',
'relatively',
'regarding',
'really',
'res',
'respecting',
'respectively',
'q',
'agent',
'quite',
'que',
'qua',
'n',
'neaths',
'neath',
'nethe',
'nethermost',
'nigh',
'nighest',
'nigher',
'nine',
'noone',
'nobodies',
'nowhere',
'nowheres',
'noes',
'nos',
'no-one',
'nothings',
'nothing',
'nathless',
'natheless',
't',
'ten',
'tills',
'till',
'tilled',
'tilling',
'to',
'towards',
'toward',
'towardest',
'towarder',
'together',
'too',
'thy',
'thyself',
'thus',
'than',
'that',
'those',
'thou',
'though',
'thous',
'thouses',
'thru',
'thruer',
'thruest',
'thro',
'through',
'throughout',
'throughest',
'througher',
'thine',
'this',
'thises',
'they',
'thee',
'the',
'then',
'thence',
'thenest',
'thener',
'them',
'themselves',
'these',
'therer',
'there',
'thereby',
'therest',
'thereafter',
'therein',
'thereupon',
'their',
'theirs',
'thing',
'things',
'three',
'two',
'the',
'o',
'oh',
'owt',
'owning',
'owned',
'own',
'owns',
'others',
'other',
'of',
'offest',
'one',
'ought',
'oughts',
'our',
'ours',
'ourselves',
'ourself',
'out',
'outest',
'outed',
'outwith',
'outs',
'outside',
'over',
'overs',
'or',
'orer',
'orest',
'on',
'oneself',
'onest',
'ons',
'onto',
'a',
'atween',
'at',
'athwart',
'atop',
'afore',
'afterward',
'afterwards',
'after',
'afterest',
'afterer',
'ain',
'an',
'any',
'anything',
'anybody',
'anyone',
'anyhow',
'anywhere',
'anent',
'anear',
'and',
'andor',
'another',
'around',
'ares',
'are',
'aest',
'aer',
'accordingly',
'abaft',
'abafter',
'abaftest',
'abovest',
'above',
'abover',
'abouter',
'aboutest',
'about',
'aid',
'amidst',
'amid',
'along',
'alongside',
'allest',
'aller',
'allyou',
'alls',
'albeit',
'as',
'aside',
'asides',
'aslant',
'ases',
'astrider',
'astride',
'astridest',
'astraddlest',
'astraddler',
'astraddle',
'aughts',
'aught',
'vs',
'v',
'via',
'vis-a-vis',
'vis-a-viser',
'vis-a-visest',
'viz',
'veriest',
'verier',
'versus',
'k',
'g',
'go',
'got',
'gotta',
'gotten',
'get',
'gets',
'getting',
'b',
'by',
'byandby',
'by-and-by',
'bist',
'both',
'buts',
'be',
'beyond',
'because',
'became',
'becomes',
'become',
'becoming',
'becomings',
'becominger',
'becomingest',
'behind',
'behinds',
'before',
'beforehand',
'beforehandest',
'beforehander',
'bettered',
'betters',
'better',
'bettering',
'betwixt',
'between',
'beneath',
'been',
'below',
'besides',
'beside',
'm',
'my',
'myself',
'mucher',
'muchest',
'musts',
'musths',
'musth',
'main',
'make',
'mayest',
'many',
'mauger',
'maugre',
'me',
'VirginAmerica',
'meanwhiles',
'meanwhile',
'moreover',
'might',
'mights',
'midst',
'midsts',
'h',
'huh',
'humph',
'he',
'hers',
'herself',
'her',
'hereby',
'herein',
'hereafters',
'hereafter',
'hereupon',
'hence',
'having',
'haves',
'hast',
'hardly',
'hae',
'hath',
'him',
'himself',
'hither',
'hitherest',
'hitherer',
'his',
'how-do-you-do',
'however',
'how',
'howbeit',
'howdoyoudo',
'hoos',
'hoo',
'w',
'woulded',
'woulding',
'we',
'wert',
'with',
'withal',
'without',
'within',
'why',
'what',
'whatever',
'whateverer',
'whateverest',
'whatsoeverer',
'whatsoeverest',
'whatsoever',
'whence',
'whencesoever',
'whenever',
'whensoever',
'when',
'whenas',
'whether',
'wheen',
'whereto',
'whereupon',
'wherever',
'whereon',
'whereof',
'where',
'whereby',
'wherewithal',
'wherewith',
'whereinto',
'wherein',
'whereafter',
'whereas',
'wheresoever',
'wherefrom',
'which',
'whichever',
'whichsoever',
'whilst',
'while',
'whiles',
'whithersoever',
'whither',
'whoever',
'whosoever',
'UnitedAppeals',
'whoso',
'whose',
'whomever',
's',
'syne',
'syn',
'shalling',
'shall',
'shalled',
'shalls',
'shoulding',
'she',
'sayyid',
'sayid',
'said',
'saider',
'saidest',
'same',
'samest',
'sames',
'samer',
'saved',
'sans',
'sanses',
'sanserifs',
'sanserif',
'so',
'soer',
'soest',
'sobeit',
'someone',
'somebody',
'somehow',
'some',
'somewhere',
'somewhat',
'something',
'sometimest',
'sometimes',
'sometimer',
'several',
'american',
'severaler',
'severalest',
'seriousest',
'seriouser',
'senza',
'send',
'sent',
'seem',
'seems',
'seemed',
'seemingest',
'seeminger',
'seemings',
'seven',
'summat',
'sups',
'sup',
'supping',
'supped',
'such',
'since',
'sine',
'sines',
'sith',
'six',
'stop',
'stopped',
'p',
'plaintiff',
'plenty',
'plenties',
'please',
'pleased',
'pleases',
'per',
'perhaps',
'particulars',
'particularly',
'particular',
'particularest',
'particularer',
'pro',
'providing',
'provides',
'provided',
'EZEE',
'provide',
'probably',
'l',
'layabout',
'layabouts',
'latter',
'latterest',
'latterer',
'latterly',
'latters',
'lotting',
'lotted',
'lot',
'lest',
'ie',
'ifs',
'if',
'i',
'info',
'information',
'itself',
'its',
'it',
'idem',
'idemer',
'idemest',
'immediate',
'immediatest',
'immediater',
'in',
'inwards',
'inwardest',
'inwarder',
'inward',
'inasmuch',
'into',
'instead',
'insofar',
'indicates',
'indicated',
'indicate',
'indicating',
'indeed',
'inc',
'f',
'fact',
'facts',
'fs',
'figupon',
'figupons',
'figuponing',
'figuponed',
'few',
'fewer',
'fewest',
'frae',
'from',
'failing',
'failings',
'five',
'furthers',
'furtherer',
'furthered',
'furtherest',
'further',
'furthering',
'furthermore',
'fourscore',
'followthrough',
'for',
'forwhy',
'fornenst',
'formerly',
'former',
'formerer',
'formerest',
'formers',
'forbye',
'forby',
'fore',
'forever',
'forer',
'fores',
'four',
'd',
'ddays',
'dday',
'do',
'doing',
'doings',
'doe',
'doth',
'downwarder',
'downwardest',
'downward',
'downwards',
'downs',
'done',
'doner',
'dones',
'donest',
'dos',
'dost',
'differentest',
'differenter',
'different',
'describing',
'describe',
'describes',
'described',
'despiting',
'despites',
'despited',
'despite',
'during',
'c',
'cum',
'america',
'circa',
'chez',
'cer',
'certain',
'certainest',
'certainer',
'cest',
'canst',
'cants',
'canting',
'cantest',
'canted',
'co',
'could',
'couldst',
'comeon',
'comeons',
'come-ons',
'come-on',
'concerning',
'concerninger',
'concerningest',
'consequently',
'considering',
'e'
'eg',
'eight',
'either',
'even',
'evens',
'evenser',
'evensest',
'evened',
'evenest',
'ever',
'everyone',
'everything',
'everybody',
'everywhere',
'every',
'ere',
'each',
'et',
'etc',
'elsewhere',
'else',
'ex',
'excepted',
'excepts',
'except',
'excepting',
'exes',
'enough',
'@VirginAmerica',
'@carrieunderwood',
'@ladygaga',
':/',
'@united',
'@United',
'@',
'OGG',
'EZEE',
'CSR',
'',
'SNA',
'SFO',
'SFO',
'EWR',
'&amp',
'just',
'youto',
'youve',
'youyour',
'youî',
'yow',
'ypu',
'yr',
'yrs',
'yu',
'yuck',
'yuen',
'yulordit',
'yvr',
'yxe',
'yxu',
'yyc',
'yyj',
'yyz',
'yî',
'zaidan',
'zam',
'zany',
'zelda',
'zhang',
'zrh',
'zrhairport',
 'zukes',
 'zurich',
 'zwick',
 'ªsfo',
 '²î',
 '³he',
 'ºhouston',
 '¼l',
 '¾ï',
 'ws',
 'wry',
 'wontflyagainwithyou',
 'wonî',
 'wks',
 'widow',
 'widowmaker',
 'wiel',
 'wife',
 'americanair',
 'wifes',
 'wifey',
 'wifi',
 'wifiand',
 'wifihotel',
 'wifimissed',
 'whyairtravelsucks',
 'whyany',
 'wht',
 'werin',
 'werner',
 'wertmuller',
 'wesley',
 'waysilverairwsys',
 'wayward',
 'wayway',
 'wc',
 'wcancelled',
 'vic',
 'vin',
 'vi',
 'va',
 'ut',
 'usairways',
 'usairwaysfail',
 'usas',
 'usd',
 'uhuhgroup',
 'uk',
 'umm',
 'ump',
 'un',
 'una',
 'tur',
 'turf',
 'turgid',
 'ts',
 'tsa',
 'tsai',
 'tshirt',
 'tsk',
 'cpu',
 'usb',
  'internet',
 'his', 'school', 'assignments',
 'tso',
 'tu',
 'truetolife',
 'trimmings',
 'trio',
 'triessohardto',
 'triedandtrue',
 'trafficfuel',
 'tonysimsmma',
 'tongueincheek',
 'tomorrowwhy',
 'tkt',
 'tkts',
 'tminus',
 'tmm',
 'tmrw',
 'tnx',
 'tix',
 'tks',
 'sop',
 'snit',
 'snl',
 'sno',
 'smf',
 'smh',
 'size',
 'sizzle',
 'sjo',
 'sjoiad',
 'sivi',
 'shaw',
 'sf',
 'sfo',
'sfo',
'sfoewr',
'sfofll',
'sfogtbos',
'sfogtyyz',
'sfolax',
'sfolaxsalclo',
'sfonew',
'sfopdx',
'sfosea',
'sfotobos',
'sfoyyj',
 'serv',
 'ser',
 'sep',
 'runningonthreehoursofsleep',
 'rougharoundtheedges',
 'roadwarrior',
 'pricediscrimination',
 'pointandshoot',
 'pm',
 'plannedneverflyvirginfor',
 'oma',
 'omaha',
 'ode',
 'oedekerk',
 'nws',
 'nxt',
'ny',
 'nyc',
 'nycjfk',
 'nyfw',
 'nytimes',
 'nu',
 'nrt',
 'nt',
 'ntrustopen',
 'noooooooooooooooooooooope',
 'nippon',
 'niro',
 'nit',
 'nite',
 'nj',
 'nm',
 'newarkbrussels',
 'newarklets',
 'newarklondon',
 'newarktiredandwanttogohome',
 'neverflyunited',
'neverflyvirgin',
 'neverflyvirginfor',
 'nerverattling',
 'needtocatchmynextflight',
 'nawww',
 'nashville',
 'myvxexperience',
 'na',
 'nada',
 'naelah',
 'nah',
 'naiamiaa',
 'nail',
 'multipledooropeningandclosing',
 'mpls',
 'mr',
 'ms',
 'msg',
 'msn',
 'msp',
 'msy',
 'mtgs',
 'mths',
 'mtn',
 'muc',
 'motel',
 'mother',
 'motherdaughter',
 'mothman',
 'morsels',
 'jimtrotternfl',
 'jj',
 'jk',
 'jkf',
 'jezziegoldz',
 'jfk',
 'jfkgtsfo',
 'jfklax',
 'jfklhr',
 'jh',
 'jhalways',
 'jilted',
 'jettisoned',
 'jeffsmisek',
 'jagged',
 'jagjit',
 'jaglom',
 'itî',
 'ivan',
 'ive',
 'iy',
 'iî',
 'itll',
 'ito',
 'itpretend',
 'itpro',
 'istan',
 'isthisyourfirsttry',
 'iq',
 'inï',
 'iha',
 'ii',
 'il',
 'ilk',
 'iflyalot',
 'iflyoakland',
 'ig',
 'idnum',
 'iad',
 'iadsat',
 'iah',
 'iahmnl',
 'iahwowjust',
 'iamtedking',
 'ian',
 'iatan',
 'httpstcooardjjgrrd',
 'httpstcozgoqoxjbqy',
 'httptco',
 'httptcoahlxhhkiyn',
 'httptcoaoeaeszdlx',
 'httptcoav',
 'httptcoavrtowtyzk',
 'httptcobeqotlnugc',
 'httptcoc',
 'httptcodajwzhlvyu',
 'httptcodnstitrzwy',
 'httptcodxicoyioxf',
 'httptcoewxwxidtfx',
 'httptcofgr',
 'httptcofjkvqm',
 'httptcoflwmgdahxu',
 'httptcogkgkzlawpr',
 'httptcoj',
 'httptcojloi',
 'httptcojnqn',
 'httptcojorescf',
 'httptcokcqnwixucm',
 'httptcokcvu',
 'httptcokvfwajvvid',
 'httptcolulgnweffh',
 'httptcom',
 'httptcomarcnocwzn',
 'httptcomjkpgvxmpc',
 'httptcomvyoizrpde',
 'httptconm',
 'httptconvlnglnmgn',
 'httptcoowmaxoyehz',
 'httptcopnbajfkmhg',
 'httptcopxexilsjbs',
 'httptcoqde',
 'httptcoqkquraggoo',
 'httptcorgywjb',
 'httptcosjqemdtqma',
 'httptcotkauygcpms',
 'httptcotmccexyaaq',
 'httptcotrqlpeinzw',
 'httptcoukdjjijrow',
 'httptcovdfdodqvgx',
 'httptcovrqdpqepfw',
 'httptcox',
 'httptcoxatoxbnsfa',
 'httptcoxijyrpslzk',
 'httptcoxzbajmiekx',
 'httptcoy',
 'airport',
 'also',
 'httptcozsuztnaijq',
 'houstonbogota',
 'SFOgtYYZ',
 'houstonjust',
 'hourandahalf',
 'hourandahalflong',
 'hourandtenminutedelay',
 'hopethegearmakesitintact',
'hopetogetanswersoon',
'I',
'about',
'an',
'are',
'as',
'at',
'be',
'by',
'com',
'for',
'from',
'how',
'in',
'it',
'of',
'on',
'or',
'that',
'the',
'this',
'to',
'what',
'when',
'where',
'who',
'will',
'with',
'the',
'www',
 'hooliganism',
 'hn',
 'The',
 'the',
 'FLL',
 'Ewr',
 'an',
 'there',
 'hnl',
 'hitchcock',
 'hitchcockian',
 'hithungry',
 'hits',
 'hkg',
 'hmm',
 'hmmmseems',
 'hippopotamus',
 'helpunitedsucks',
 'hemisphere',
 'hemispheres',
 'hemispheresmag',
 'hedonistic',
 'headerelevateusernumofpointsavaila',
 'havenî',
 'gsa',
 'gt',
 'greetingz',
 'goldsspousaldiscriminationangry',
 'gnvltltltfsd',
 'givethoseladiesraise',
 'getmeoffrhisfuckinplane',
 'ft',
 'ftlauderdalesun',
 'ftw',
 'fu',
 'fritz',
 'frm',
 'fro',
 'frolic',
 'from',
 'fromtelevision',
 'freshsqueezed',
 'freudianism',
 'freyabevanfund',
 'firstclass',
 'firstrate',
 'firsttimer',
 'firstworldpro',
 'filming',
 'filmmaker',
 'filmmakers',
 'filmmaking',
 'findanothergate',
 'fete',
 'fetishes',
 'ff',
 'fi',
 'fiancç',
 'fiasco',
 'fiascos',
 'feetî',
 'feisty',
 'fell',
 'fella',
 'fcmostinnovative',
 'fe',
 'fb',
 'fc',
 'faundation',
 'fauxur',
 'fastcompanys',
 'fargo',
 'fargoairport',
 'fampampcking',
 'fampking',
 'famuyiwa',
 'familyfriendly',
 'familywe',
 'falseadvertising',
'eyes',
 'ezee',
 'fa',
 'faa',
 'exu',
 'exude',
 'exxon',
 'ey',
 'exp',
 'exhi',
 'eu',
 'eticket',
 'touchscreen',
 'college',
 'next',
 'year', 'want', 'give', 'him', 'her', 'she','he', 'mother', 'father', 'brother', 'sister',
 'law',
 'wi-fi', 'apple', 'android',
 'operating', 'system', 'fire',
 'hd',
 'tablet',
 'tablets','almost', 'gig', 'yr', 'old', 'year', 'amazon', 'customer', 'netflix', 'airplane', 'services', 'service',
 'germany', 'china', 'inch', 'looooooong', 'buy', 'bought', 'son', 'family', 'daughter', 'very', 'fiance',
 'etihad',
 'eu',
 'eta',
 'ers',
 'erupt',
 'escapade',
 'escape',
 'escapism',
 'escapist',
 'eschews',
 'escorted',
 'eserviceunitedcom',
 'esp',
 'er',
 'era',
 'eps',
 'eqms',
 'enuf',
 'englishspanish',
 'en',
 'ena',
 'emilydonneiiy',
 'elp',
 'elpaso',
 'elses',
 'elusive',
 'em',
 'elingeniero',
 'elite',
 'eliza',
 'ellas',
 'ellen',
 'eh',
 'ehsanismpowered',
 'eileen',
 'either',
 'el',
 'ela',
 'eg',
 'edward',
 'eerie',
 'ed',
 'edgy',
 'edin',
 'edit',
 'ea',
 'dv',
 'dvd',
 'dvr',
 'dvt',
 'dry',
 'dtv',
 'dtw',
 'dtwase',
 'du',
 'droppeditoffyet',
 'dpt',
 'dpted',
 'dr',
 'downanddirty',
 'downer',
 'downmy',
 'downright',
 'downtoearth',
 'dounotwantmybusiness',
 'dontdothistome',
 'dong',
 'donkey',
 'donna',
 'donovan',
 'doesntfeellikestatusyet',
 'doesnâ',
 'doesnî',
 'dj',
 'dm',
 'dmangenvisualclu',
 'dmcome',
 'dmd',
 'dmed',
 'dming',
 'dms',
 'doa',
 'ditsy',
 'disastertravelingwithsmallkids',
 'disappointmentyet',
 'disappearingreappearing',
 'disaffectedindiefilm',
 'disa',
 'directtovideo',
 'dinosaurplane',
 'dinosaurplane',
 'difficultieswhat',
 'didntå',
 'didnî',
 'didactic',
 'dey',
 'dfpietra',
 'dfw',
 'dfwlax',
 'devito',
 'despica',
 'despise',
 'dest',
 'dep',
 'delhi',
 'delecta',
 'delayedagainevery',
 'delayedno',
 'delayedover',
 'deice',
 'deiced',
 'deicing',
 'del',
 'deci',
 'decem',
 'dc',
 'dca',
 'de',
 'david',
 'davis',
 'dawdle',
 'dawson',
 'dangerofgettingsnowedin',
 'dan',
 'dana',
 'danahajek',
 'damper',
 'damsel',
 'dal',
 'dalaus',
 'daldca',
 'dallas',
 'dallasaustin',
 'dadoralive',
 'dahmer',
 'dai',
 'czamkoff',
 'cvg',
 'cx',
 'cxl',
 'cxp',
 'cup',
 'cur',
 'cure',
 'cun',
 'cunewr',
 'cs',
 'csfail',
 'csr',
 'css',
 'ct',
 'continentalairlines',
 'continentalunite',
 'communicationdetails',
 'communicationfail',
 'coltsmissingbags',
 'cmh',
 'cmhiad',
 'cmhord',
 'cnn',
 'cnnmoney',
 'cnx',
 'ciscojimfrench',
 'cinematography',
 'childridiculous',
 'chickenidiot',
 'chasm',
 'characterwhoshall',
 'cesspool',
 'cgi',
 'cgjase',
 'cha',
 'ceo',
 'cere',
 'cert',
 'cc',
 'ce',
 'ceases',
 'cele',
 'carwreck',
 'cary',
 'case',
 'cases',
 'caseî',
 'carvey',
 'carrieunderwood',
 'careerdefining',
 'cantlogoutofunitedwifi',
 'businessfirst',
 'businesstravel',
 'bringyourown',
 'britishairways',
 'bna',
 'bgm',
 'bhm',
 'big',
 'biggest',
 'bil',
 'bila',
 'bins',
 'bbbnesdksia',
 'bc',
 'bcn',
 'bday',
 'bdl',
 'battierccipuppy',
 'baldwin',
 'badcustomerservice',
 'attitudycustomer',
 'asus',
 'at',
 'atc',
 'ate',
 'atgate',
 'atl',
 'ase',
 'arvls',
 'as',
 'ar',
 'arab',
 'arc',
 'aok',
 'ap',
 'anna',
 'anne',
 'amy',
 'amypoehler',
 'an',
 'ana',
 'amp',
 'ampfeel',
 'ampsock',
 'ampwe',
 'ams',
 'akin',
 'al',
 'ala',
 'aisle',
 'aisles',
 'airline',
 'airlinegave',
 'airlinegeeks',
 'airlineguys',
 'airlines',
 'airlinesecurity',
 'airlinesyour',
 'airnzusa',
 'airplane',
 'airplanemodewason',
 'air',
 'aircanada',
 'aircargo',
 'aircraft',
 'aircrafts',
 'aircrft',
 'airfare',
 'airfarewatchdog',
 'ahem',
 'ahold',
 'ahttptcolwotkiekgu',
 'ai',
 'aids',
 'agtb',
 'ad',
 'ac',
 'acc',
 'abq',
 'aa',
 'aaaand',
 'aaba',
 'aaliyah',
 'aaron',
 'abc',
 'Bos&gt;Las',
 'bos&gt;las',
 'IAD',
 'Lax',
 'LAX',
 'iad',
 'lax',
 'carrieunderwood',
 'lady gaga',
 'Lady Gaga',
 'Sfo',
 'sfo',
 'DAL',
 'dal',
 'travelingwithsmallkids',
 'gg8929',
 'ROC',
 'roc',
 'rno',
 'den',
 'ord',
 'smf',
 '#tv4u',
 'shulemstern',
 'airlinegeeks',
 'RNO',
 'DEN',
 'ORD',
 'SMF',
 '#TV4U',
 'ShulemStern',
 'unitedAirlines',
 'flight',
 'AirlineGeeks',
 ' ',
'several',
'shall',
'she',
'shed',
'shes',
'show',
'showed',
'shown',
'showns',
'shows', 'ads', 'adt',
'significant',
'significantly',
'similar',
'similarly',
'since',
'six',
'slightly',
'so',
'some',
'somebody',
'somehow',
'someone',
'somethan',
'something'
'sometime',
'sometimes',
'somewhat',
'somewhere',
'soon',
'due',
'during',
'each',
'ed',
'edu',
'effect',
'eg',
'eight',
'eighty',
'either',
'else',
'elsewhere',
'end',
'monday',
'tuesday',
'wednesday',
'thursday',
'friday',
'saturday',
'sunday',

'january',
'february',
'march',
'april',
'may',
'june',
'july',
'august',
'september',
'october',
'november',
'december',
'ending',
'enough',
'especially',
'et',
'et-al',
'etc',
'even',
'ever',
'every',
'everybody',
'everyone',
'everything',
'welcome',
'went',
'what',
'whatever',
'whats',
'when',
'whence',
'whenever',
'granddaughter',
'grandson',
'where',
'whereafter',
'whereas',
'whereby',
'wherein',
'wheres',
'whereupon',
'wherever',
'Äôs',
'äôs',
'Äôll',
'äôll',
'Alexa',
'alexa'
'whether',
'which',
'while',
'whim',
'whither',
'who',
'whod',
'whoever',
'utube',
'whole',
'whom',
'whomever',
'whos',
'whose',
'christmas',
'why',
'app',
'itâ',
'brief',
'play',
'music', 'echoâ','echo',
'pandora',
'musicâ', 'bulb', 'kitchen', 'room', 'toilet',
'ã',
'didnâ ã ã', 'box', 'connection'
'didnâ',
'children',
'soninlaw',
'alexa',
'hub', 'use', 'plugs',
'plug',
'device',
'elderly',
'goodagood', 'accuratei',
'screen',
'saw',
'birthday',
'moms','dads','dad','mom',
'see',
'sea',
'paperwhite','siriusxm','songs', 'song'
]

#clean data

# text cleaning of the review
def clean_text(txt):
    txt = "".join([c for c in txt if c not in string.punctuation])  # remove the punctuation
    # print(txt)
    txt = re.sub(r'@[A-Za-z0-9]+', '', txt)
    txt = re.sub(r'b\'[A-Za-z0-9]+', '', txt)
    txt = re.sub(r'%[A-Za-z0-9]+', '', txt)
    txt = re.sub(r'$[A-Za-z0-9]+', '', txt)
    txt = re.sub(r'#[A-Za-z0-9]+', '', txt)
    txt = re.sub(r'&[A-Za-z0-9]+', '', txt)
    txt = re.sub(r'\w*\d\w*', '', txt).strip()
    txt = re.sub(r'#', '', txt)
    txt = re.sub(r'RT[\s]+', '', txt)
    txt = re.sub(r'https?:\/\/\S+', '', txt)
    tokens = re.split('\W+', txt)  # split the word          stopwords are from the above
    # print(tokens)
    i = 0

    for word in tokens:
        # print(word)
        if (word != '' and word != ' '):
            lower_word = word.lower()
            if (lower_word not in myStopwords):
                # print(word+" not in stopwords list")
                if ((lower_word.isnumeric()) == False):
                    if (containNumber(lower_word) == False):
                        if (containSign(lower_word) == False):
                            if (i == 0):
                                dummy = lower_word
                                i = i + 1
                            else:
                                dummy = dummy + " " + lower_word
        # else:
        # print(lower_word+" in stopwords list")
    # txt = " ".join([word for word in tokens if (((word not in myStopwords)==True) and (word.isnumeric()==False))])# get rid of the stop word # change to my stop word
    # space between words
    # word not in stopwords is not working because booleans, need == for boolean
    # booleans != boolean
    txt = dummy
    # print("cleaned text: "+ dummy)
    dummy = re.split('\W+', txt)
    # print("length of the text: "+ str(len(dummy)-1))
    return txt


def containNumber(txt):
    contains_digit = False
    for ch in txt:
        if (ch.isdigit()):
            contains_digit = True
            break
    return contains_digit


def containSign(txt):
    contains_sign = False
    for ch in txt:
        if (ch == '&' or ch == '/' or ch == ':' or ch == ';'
                or ch == '-' or ch == '=' or ch == '<' or ch == '>' or ch == '%' or ch == '+'
                or ch == '_' or ch == 'ä' or ch == 'ü' or ch == 'ö' or ch == '#' or ch == "^"):
            contains_sign = True
            break
    return contains_sign


def max_text(txt, dataset_length):
    max = 0;
    # print(txt.shape[0])
    for i in range(dataset_length):
        dummy = re.split('\W+', txt[i])
        if (len(dummy) > max):
            max = len(dummy)
    return max

comment_dataset['comment'] = comment_dataset['comment'].apply(lambda x: clean_text(x))

# Get the sentences and the labels
# for both the training and the validation sets
training_reviews = []
training_labels = []

validation_reviews = []
validation_labels = []

# input excel dataset into training_review
len_of_reviews = 0
index = 0
store_index = 0
# input into training and validation set
training_len = 5867
validation_len = 2515

for i in range(training_len):
    item = comment_dataset['comment'][i]
    label = comment_dataset['label'][i]
    #print(item + " " + str(comment_dataset['label'][i]))
    training_reviews.append(item)
    training_labels.append(label)
    # print("length of word "+ str(len(str(review.numpy()))))
    token = re.split('\W+', item)
    if (len(token) > len_of_reviews):
        len_of_reviews = len(token)
        store_index = index
    index = index + 1

print("\nNumber of training reviews is: ", len(training_reviews))
print("\n max largest length of reviews is: " + str(len_of_reviews))
print("\n stored index is " + str(store_index))
print(" index: " + str(store_index) + " " + training_reviews[store_index])

for i in range(validation_len):
    item = comment_dataset['comment'][i + training_len]
    label = comment_dataset['label'][i + training_len]
    print(item + " " + str(comment_dataset['label'][i + training_len]))
    validation_reviews.append(item)
    validation_labels.append(label)
    # print("length of word "+ str(len(str(review.numpy()))))


# Take 10,000 reviews
len_of_reviews = 0
index = 0
store_index = 0

i = 0
for item in comment_dataset['comment']:
  print(item+" "+str(comment_dataset['label'][i]))
  label = comment_dataset['label'][i]
  training_reviews.append(item)
  training_labels.append(label)
  #print("length of word "+ str(len(str(review.numpy()))))
  token = re.split('\W+', item)
  if(len(token)>len_of_reviews):
    len_of_reviews = len(token)
    store_index = index
  index = index + 1
  i = i+1

#see the single gram
df = pd.DataFrame(training_reviews, columns=['review'])

dict = {'training_reviews': training_reviews}
df = pd.DataFrame(dict)
my_df.to_csv("/training_review.csv", index = False)


vocab_size = 7000  #3000 change dictionary .length
embedding_dim = 100 #change 16 to 64
max_length = 87  #50 max length and turnc type are to padding the sentence
trunc_type='post'
pad_type='post'
oov_tok = "<OOV>"

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok) #turn the number into number, and oov is to store the words never seen
tokenizer.fit_on_texts(training_reviews) #change the word to index
word_index = tokenizer.word_index


print ("\nNumber of training reviews is: ", len(training_reviews))
print ("\n max largest length of reviews is: " + str(len_of_reviews))
print("\n stored index is "+ str(store_index))
print(" index: "+ str(store_index)+" "+training_reviews[store_index])

import tensorflow.compat.v2 as tf
tf.enable_v2_behavior()

# Get the dataset.
# It has 70000 items, so might take a while to download
dataset, info = tfds.load('glue/sst2', with_info=True)
print(info.features)
print(info.features["label"].num_classes)
print(info.features["label"].names)

# Get the training and validation datasets
dataset_train, dataset_validation = dataset['train'], dataset['validation']

# Pad the sequences so that they are all the same length
training_sequences = tokenizer.texts_to_sequences(training_reviews)
training_padded = pad_sequences(training_sequences,maxlen=max_length,
                                truncating=trunc_type, padding=pad_type)
validation_sequences = tokenizer.texts_to_sequences(validation_reviews)
validation_padded = pad_sequences(validation_sequences,maxlen=max_length, truncating=trunc_type, padding=pad_type)

training_labels_final = np.array(training_labels)
validation_labels_final = np.array(validation_labels)


def plot_graphs(history, string):
    plt.plot(history.history[string])
    plt.plot(history.history['val_' + string])
    plt.xlabel("Epochs")
    plt.ylabel(string)
    plt.legend([string, 'val_' + string])
    plt.show()


def predict_review(model, reviews):
  # Create the sequences
  padding_type='post'
  sample_sequences = tokenizer.texts_to_sequences(reviews)
  reviews_padded = pad_sequences(sample_sequences, padding=padding_type,
                                 maxlen=max_length)
  print(type(reviews_padded))
  classes = model.predict(reviews_padded)
  for x in range(len(reviews_padded)):
    print(reviews[x])
    print(classes[x])
    print('\n')

def fit_model_and_show_results (model, reviews):
  model.summary()
  history = model.fit(training_padded, training_labels_final, epochs= 18,
                      validation_data=(validation_padded, validation_labels_final))
  plot_graphs(history, "accuracy")
  plot_graphs(history, "loss")
  predict_review(model, reviews)
  return history, model


# Write some new reviews

review1 = """This is not so bad"""

review2 = """the trade can be better"""

review3 = """The user is not so friendly"""

review4 = """The bottle is broken but user report honestly"""

new_reviews = [review1, review2, review3, review4]


model_multiple_bidi_lstm = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim, return_sequences=True)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
#added dropout function
learning_rate = 0.00003 # changed the learning rate to 0.1 from 0.0003
#change again from 0.1 to 0.00001
model_multiple_bidi_lstm.compile(loss='binary_crossentropy',
          optimizer=tf.keras.optimizers.Adam(learning_rate),
                                 metrics=['accuracy'])

history, model = fit_model_and_show_results(model_multiple_bidi_lstm, new_reviews)

predict_review(model_multiple_bidi_lstm,  new_reviews)






